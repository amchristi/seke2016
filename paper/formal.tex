\section{Assumptions and Guarantees}
\label{formal}

In addition to being orthogonal to the spectrum-based localization
formula used, test case reduction has a second major advantage
independent of empirical results.  Namely, under a set of assumptions
that hold in many cases, \emph{reduction can only improve, or leave
unchanged, the effectiveness of localization}.  All formulas for
localization have some instances in which they diminish the
effectiveness of localization compared to an alternative formula
\cite{yoo2014no}. 
Reducing failing tests before applying a formula, however, at worst
leaves the effectiveness of localization unchanged, for most of the
formulas in widespread use that we are aware of, under three
assumptions:

\begin{enumerate}
\item all failing test cases used in the localization involve the same
fault,
\item each failing test case reduces to a test case that fails due to the same fault as the original test case, and
\item reducing the input size (in components) also covers less code when the test executes.
\end{enumerate}

The first assumption is probably the least likely to hold in some
settings; however, it is also the assumption that is least relied
upon. The second assumption is a usual assumption of delta-debugging.  Most
delta-debugging setups are engineered with this goal in mind, often
using some aspect of failure output or test case structure to keep
``the same bug.''  Observed ``slippage'' rates for faults
seem to be fairly low \cite{PLDI13}, even with little mitigation, and
mitigation strategies have been proposed to reduce even this rate \cite{slippageMit}.
Note that in the setting where a program has a single fault,
assumptions 1 and 2 always hold.  As to the third assumption, it is
uncommon but possible for reduced test cases to increase coverage;
cause reduction can usually be used to mitigate the rare exceptions \cite{icst14}.

Given these assumptions, we now show that reduction is, at worst,
harmless for most formulas.  Recall that spectrum-based localizations
rely on only a few values relevant to each entity $e$ to be ranked in
a localization: $\text{passed}(e)$, $\text{failed}(e)$,
$\text{totalpassed}$, and $\text{totalfailed}$.  Given assumptions 1-3
above, \emph{for faulty statements all of these formula elements will
be unchanged after delta-debugging}.  For non-faulty statements, the
only possible change is that $ \text{failed}(e)$ may be lower than
before failing test cases were reduced. Holding the other values
constant, it is trivial to show that most formulas under consideration
are (as we would expect), monotonically increasing in
$\text{failed}(e)$.  Therefore, after reduction, the suspiciousness
scores for faulty statements are unchanged and the suspiciousness
scores for non-faulty statements are either unchanged or lower.  The
rank of all faulty statements is therefore either unchanged or
improved.  There are many spectrum-based fault localization formulas
in use. In our evaluation, we have used 3 well known examples, in
addition to the basic Tarantula \cite{Jones2002} formula, as
representative:

{\bf Ochiai \cite{Ochai}:}
$$ \text{suspiciousness}(e) = \frac{\text{failed}(e)}{\sqrt{(\text{totalfailed}) (\text{failed}(e) + \text{passed}(e))}} $$

{\bf Jaccard \cite{Pinpoint}:}
$$ \text{suspiciousness}(e) = \frac{\text{failed}(e)}{\text{failed(e)} + \text{totalfailed}} $$

{\bf SBI: \cite{EmpirReduce,StatDebug}}
$$ \text{suspiciousness}(e) = \frac{\text{failed}(e)}{\text{failed(e)} + \text{passed}(e)} $$


All these formulas are monotonically increasing in
$\text{failed}(e)$.%\footnote{Proofs available on request, and to be  posted on our web page as an appendix.} 
Reduction can only
theoritically improve fault localization or in worst case leave it
unchanged if the formula under consideration is monotonically
increasing $\text{failed}(e)$. We chose these 4 formulas as they were
used before to study the effects of test suite reduction
\cite{EmpirReduce} and test case purification
\cite{PureTest} on fault localization.



