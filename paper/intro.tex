\section{Introduction}
Debugging is one of the most time-consuming and difficult aspects of
software development \cite{Vesey,BallVis}.  Recent years have seen a
wide variety of research efforts devoted to easing the burden of
debugging by \emph{automatically localizing faults}.  The most popular
approaches, following the seminal work of Jones, Harrold, and Stasko
\cite{Jones2002,Tarantula} use statistics of spectra \cite{RepsSpectra} of
failing and successful executions to score program entities according
to how likely they are to be faulty.  These spectrum-based approaches
are popular in part because they have outperformed competing
approaches, and in part because they are highly efficient and easy to
use --- they typically only require the collection of coverage data
and marking of tests as passing and failing, and thus are both
computationally cheap and easy to fully automate.  Many formulas have
been proposed as potentially improving the accuracy of scores
\cite{Ochai,AMPLE,Pinpoint,StatDebug,Abreu:2006:PRDC} over Tarantula.

Despite this large body of work and continuing interest, there is recent concern about the long-term value
of localization research.  Parnin and Orso asked the core
question: ``Are automated debugging techniques actually helping
programmers?''  \cite{AutoHelp}, and did not receive comforting
answers.  Parnin and Orso studied how actual programmers made use of localization techniques \cite{AutoHelp}
and concluded that 1) absolute rank should be used to
measure effectiveness, because developers lose interest in
localizations after a very few incorrect suggestions, and 2) there should be a focus on
using richer information (e.g. actual test cases) rather than just
a raw localization in debugging aids.  Combined with Yoo et al.'s
establishment \cite{yoo2014no} that there is no truly optimal formula
for localization, this suggests that the most valuable contributions
to localization would be \emph{formula-independent} methods that
\emph{potentially result in extremely large improvements in fault
rank} rather than small, incremental average improvements in rank.
This paper, therefore, argues that in many cases there \emph{is} a
simple, easily applied, improvement to localization that works with
any formula (or other modification to the method we are aware of), has
benefits to developers even if they ignore the localization, and often
produces very large improvements in what we consider the most
important quantitative measure of localization effectiveness, the absolute worst
possible ranking of the faulty code.

\subsection{Reduce Before You Localize}
Failing test cases usually execute much more non-faulty code than
faulty code, and in a sense it is essentially this fact that makes fault localization difficult. Due to the way spectrum-based localizations work,
reducing the amount of non-faulty code executed in failing test cases
should almost always improve localization.  Consider the Tarantula
\cite{Jones2002,Tarantula} formula.  Tarantula, like most spectrum-based
approaches, determines how suspicious (likely to be faulty) a coverage
entity $e$ (typically a statement) is based on a few values computed over
a test suite:

\begin{itemize}
\item $\text{passed}(e)$:  \# of tests covering $e$ that pass
\item $\text{failed}(e)$:  \# of tests covering $e$ that pass
\item $\text{totalpassed}$:  the \# of passing tests
\item $\text{totalfailed}$:  the \# of failing tests
\end{itemize}

$$ \text{suspiciousness}(e) =  \frac{\frac{\text{failed}(e)}{\text{totalfailed}}}{\frac{\text{failed}(e)}{\text{totalfailed}} + \frac{\text{passed}(e)}{\text{totalpassed}}}$$


It is easy to see that if we lower $\text{failed}(e)$ for all
non-faulty statements, while keeping everything else unchanged, the
rank (in suspiciousness) of faulty statements will improve.   Reducing coverage of non-faulty
statements in failing tests, then, is a potentially very effective  and
formula-independent approach to improving localizations.

Unfortunately, there is no method we know of in the literature for reducing the
amount of non-faulty code executed in a test.  However, there \emph{is} a widely
used method for reducing the \emph{size} of failing test cases, delta-debugging.

Delta-debugging \cite{DD} (DD for short) is an algorithm for reducing
the size of failing test cases.  Delta-debugging algorithms have
retained a common core since early proposals \cite{DDISSTA}: use a
variation on binary search to remove individual components of a
failing test case $t$ to produce a \emph{new} test case $t_{1min}$
satisfying two properties: (1) $t_{1min}$ fails and (2) removing any
component from $t_{1min}$ results in a test case that does not fail.
Such a test case is called \emph{1-minimal}. 
Delta-debugging reduces the size of a test case in terms of its
components.  Its purpose is to produce small test cases that are
easier for humans to read and understand, and thus debug.  In our long
experience with delta-debugging \cite{ICSEDiff,AMAI} and in recent
work on variations and applications of delta-debugging
\cite{icst2014,issta14,PLDI13,NonAdeq,OneTest}, we noticed that in addition to
reducing the ``static'' human-readable text of a test case,
delta-debugging also almost always \emph{reduces the code covered by a
failing test case}, often by hundreds or thousands of lines
\cite{icst2014}.  The core proposal of this paper, therefore, is that
\emph{failing test cases should be, when possible, reduced with
delta-debugging before they are used in spectrum-based fault
localization}: {\bf reduce before you localize}.  The original,
unreduced, test cases should not be used, as they likely contain much
irrelevant, non-faulty code that may mislead localization.

Even if reduction does not improve the localization, we
show that under some reasonable assumptions it will not produce a
\emph{worse} localization, and at least the developer now has a set of
smaller, easier-to-understand test cases to read.  In fact, we believe
\emph{the only reason not to reduce test cases before localization, as a
best practice, is when it is too onerous (or not possible) to set up
delta-debugging for test cases.}

In this paper, we show that using delta-debugging to reduce failing
test cases, when applicable, does usually produce improvements in
fault ranking, using a variety of standard localization formula from
the literature, and these improvements are often dramatic.  In order
to place our results on a firm empirical footing \cite{Threats} we
provide results over both SIR/Siemens \cite{Siemens} suite subjects
studied in previous literature, a set of real faults from an
industrial-strength random testing framework for the SpiderMonkey
JavaScript engine \cite{icst2014,jsfunfuzz}, and a variety of open
source Java programs.  Not only does reducing failing tests produce
improvements; the improvements produced are often even better than
those provided by optimally switching formula.  
