\section{Experimental Results}
\label{sec:experiments}

\subsection{Evaluation Measure}

Because we aim to take into account the findings of Parnin and Orso
\cite{AutoHelp}, our evaluation of fault localizations is based on a
\emph{pessimistic absolute rank} of the highest ranked faulty
statement.  That is, for each set of suspiciousness metrics computed,
our measure of effectiveness is the \emph{worst possible position} at
which the first faulty statement can be reached, when examining the
code in suspiciousness-ranked order.
\footnote{We consider reaching \emph{any} faulty statement to be sufficient, as in \cite{NearNeighbor}.}
%\footnote{As usual since at least the work of Reneiris and Reiss\cite{NearNeighbor}, we consider reaching \emph{any} faulty statement to be sufficient.}  
For example,
if ten statements all receive a suspiciousness score of 1.0 (the
highest possible suspiciousness), and one of these is the fault, we
assign this localization a rank of 10; an unlucky programmer might
examine this statement last of the ten highest-ranked statements.
Pessimistic rank nicely distinguishes this result from another
localization that also places the bug at score 1.0, but gives twenty
statements a 1.0 score. In our view, following Parnin and Orso
\cite{AutoHelp}, the most important goal of a localization is to
direct the developer to a faulty statement as rapidly as possible,
ignoring the size of the entire program or even of the faulty
execution.  

\subsection{SIR Programs}
\input{sir}

\subsection{SpiderMonkey JavaScript Engine}
\input{spidermonkey}

\subsection{Open Source Projects}
\label{sec:opensource}
\input{opensource}


\subsection{Threats to Validity}

The primary threats to validity here are to external validity
\cite{Threats}, despite our use of a reasonable number of faults and subjects.  Our subjects are all C or
Java programs, for example, and for the open source projects we used
seeded rather than real faults.  To avoid
construct threats, we developed independent experimental code-bases
for some of the subjects, executed both, and compared results to
cross-check the shared code base used for all subjects.  Fortunately,
most tasks here are straightforward (test execution, coverage
collection, delta-debugging, and calculation of scores).

A second point (not strictly a threat) is that this paper focuses on
single-fault localization.    Even when
there are multiple faults, it may be better to use techniques for
clustering test cases by likely fault 
\cite{Jones07,PLDI13,Podgurski03,Podgurski04} and then perform
single-fault localization than to try to localize multiple faults at
once.
